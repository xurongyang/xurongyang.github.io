---
title: 爬虫框架设计
date: 2017-03-02 12:00:12
tags: [爬虫]
categories:
- 爬虫
---
### 起源
因为在公司做爬虫业务期间开发了一个新的基于Java的爬虫框架，故记录一下所思所想。

### 业务抽象
一个好的技术框架最重要的任务是设计出合理的抽象模型，对于爬虫框架也是一样。

如何设计爬虫系统的抽象模型呢？先看一个典型的爬虫案例。

[https://movie.douban.com/tag/喜剧](https://movie.douban.com/tag/%E5%96%9C%E5%89%A7) 是一个电影列表页，我们的任务是遍历这个列表页，进入列表页中的电影详情页，并且抓取电影详情页中的电影数据。这样的一个任务，如果不用任何爬虫框架我们应该如何实现？

思路应该是很明确的，我们用伪代码来表示：

```Java
HTML html = getFirstTagHtml()
List<Movie> items = new ArrayList()
while (true) {
    List<String> movieDetailUrls = parseUrls(html)
    for (String url : movieDetailUrls) {
        HTML detail = getMovieDetailHtml()
        Movie movie = parseMovieDetaiPage(detail)
        items.add(movie)
    }
    if (!hasNextPage()) {
        break
    }
}

```
列表-详情页模式是一种非常常见的抓取模式，这个例子对设计爬虫系统的抽象模型有何启示呢？换言之，如果再去写另外一个列表-详情页模式的抓取，是否有一些可以复用的逻辑，不用二次开发了？

答案是肯定的，总结一下有以下几点：

1. 下载页面数据
    * 异步化
    * 失败自动重试
    * 代理、cookie、userAgent等设置
2. URL调度
    * url去重
    * 分布式
3. 解析
    * 自发现规则
    * 方法回调
4. 输出

我们对爬虫的流程进行分解之后，就会发现除了一些配置之外（代理，自发现规则等），只有解析是每个任务都避免不了的，而其余的都可以复用。这就是爬虫框架的意义所在。

### 架构图
框架设计自然避免不了架构图，先贴一张架构图：

![](https://doc.scrapy.org/en/latest/_images/scrapy_architecture_02.png)

上面的图片是[Scrapy](https://github.com/scrapy/scrapy)的架构图，作为爬虫界最知名的爬虫框架，Scrapy的架构在我看来无疑是最棒的。所以，没有必要在这块重复造轮子，照搬就是。

解释一下图中的几个重要模块：

* Downloader：下载器，负责下载页面内容，处理代理，cookie等设置；
* Spider：解析器，负责解析页面内容，输入是HTML的response，输出是结构化的Items；
* Scheduler：调度器，负责调度待抓取的URL以及URL去重；
* Item Pipelines：负责处理结构化的Items数据；
* Engine：引擎，负责将上面的这几个部分连接起来一起工作；

除此之外，还有几个重要类没有体现在架构图中：

* Settings：爬虫任务配置，任何配置都只会在Settings里配置；
* Crawler：每个爬虫任务都对应一个Crawler

### 为什么要开发新的爬虫框架
按理说Scrapy已经是很不错的爬虫框架了，那么为什么又要重新开发一个呢？原因有以下几个：

1. Scrapy不支持分布式爬虫；
2. 公司的后端环境是Java环境，用Python会带来很多的兼容问题；
3. Java届最有名的爬虫框架WebMagic提供的爬虫模型不够好；
4. 爬虫框架这个轮子并不难造，并且如果是自己造的，二次扩展显然更加简便；

#### WebMagic的问题(不了解WebMagic的同学可以忽略)
先上一张架构图

![](http://code4craft.github.io/images/posts/webmagic.png)
##### PageProcessor定义混乱
PageProcessor，顾名思义，是处理页面解析内容的，对应到Scrapy中的Spider。但是，它还有两个其它的任务：1. 配置site，例如编码、抓取间隔、重试次数等；2. 启动整个爬虫任务。举个例子：

```Java
public class DoubanMovieDetailPageProcessor implements PageProcessor {

    // 配置site信息
    private Site site = Site.me().setRetryTimes(3).setSleepTime(1000);
    
    @Override
    public void process(Page page) {
        抽取页面逻辑
    }
    
    // getSite方法很多余
    @Override
    public Site getSite() {
        return site;
    }
    
    public static void main(String[] args) {
        通过Spider类启动爬虫任务
    }
}
```

很明显，无论是配置Site信息，还是启动爬虫任务都不是PageProcessor的职责。在Scrapy中，配置Site信息有Settings设置，启动爬虫任务有Crawler类，而Scrapy中的PageProcessor，也就是Spider类，只处理页面抽取，这样做逻辑要通顺很多。
##### Page类中不应该有ResultItems
ResultItems是抽取页面数据后产生的结构化数据，而Page对象是下载后的页面对象，Page类中有个ResultItems没有道理。应当和Scrapy一样，ResultItems是process方法的返回值，然后交给Pipeline处理。
##### 多页面关联解析问题
类似列表页-详情页这种模式的抓取任务，我们需要解析两个页面，列表页和详情页，用WebMagic怎么实现呢？

```Java
@override
public void process(Page page) {
    if (page.getUrl().regex(URL_LIST).match()) {
        page.addTargetRequests(page.getHtml().xpath("//div[@class=\"articleList\"]").links().regex(URL_POST).all());
        page.addTargetRequests(page.getHtml().links().regex(URL_LIST).all());
        //文章页
    } else {
        page.putField("title", page.getHtml().xpath("//div[@class='articalTitle']/h2"));
        page.putField("content", page.getHtml().xpath("//div[@id='articlebody']//div[@class='articalContent']"));
        page.putField("date",
                page.getHtml().xpath("//div[@id='articlebody']//span[@class='time SG_txtc']").regex("\\((.*)\\)"));
    }
}
```
在Scrapy中是如何实现的？

```Python
def parse(self, response):
    urls = response.xpath('//div[contains(@class, "open-pt")]//a/@href').extract()
    for url in urls:
        yield scrapy.Request(url, dont_filter = True, callback = self.parse_star)

def parse_star(self, response):
    schedule_url = response.xpath(u'//a[contains(@title, "全部行程")]/@href').extract_first()
    for month in xrange(1, 2):
        yield scrapy.Request('%s2016/%d/#blanking' %(schedule_url, month), callback = self.parse_star_schedule)

def parse_star_schedule(self, response):
    解析行程页内容
    
```
Scrapy每一个页面都对应了一个解析方法，而WebMagic则是一个Else if的判断，Scrapy明显更优。

##### 配置自发现的规则不够直接 
在PageProcessor中配置自发现的规则是这么写的：

```Java
page.addTargetRequests(page.getHtml().links().regex("(https://github\\.com/\\w+/\\w+)").all());
```
和Scrapy相比，高下立判:

```Java
rules = (
    Rule(LinkExtractor(allow=('category\.php', ), deny=('subsection\.php', ))),
    Rule(LinkExtractor(allow=('item\.php', )), callback='parse_item'),
)
```
配置这种规则并不是PageProcessor的职责，而是Site的职责，放在PageProcessor中明显不合理。

##### 并发下载问题
这是一个经典问题，在此不多说了，下载是一个IO时间远大于CPU时间的场景，用IO多路复用更合适，而不是多线程模型。

#### WebMagic的优点
说了这么多缺点，WebMagic还是有优点的。

1. 上手很快，文档比较全
2. Downloader，Scheduler，Pipeline很容易扩展
3. 代码结构比较清晰

### 开发爬虫框架时的几个难题
前面也提到了，Scrapy作为业界最佳爬虫框架，大的方面比如架构照搬就好了。但是毕竟语言有差异，很多实现细节是不能用相似方案实现的，下面主要讲几个开发爬虫框架时碰到的问题。

#### 动态初始化组件
Downloader、Scheduler和Pipeline的实现类可以由使用方替换，所以需要在爬虫任务启动时动态生成。我们希望使用方通过配置类名的方式告知实现类，而不是new出来对象再传递给Engine。这样Engine也更加容易管理各个组件，避免对象逸出。

```Java
Engine(Settings settings) throws InitComponentException {
    this.downloader = DownloaderCreator.createDownloader(settings);
    this.scheduler = SchedulerCreator.createScheduler(settings);
    this.pipeline = PipelineCreator.createPipeline(settings);
}
```
以DownloaderCreateor为例来说明初始化过程。

```Java
public static Downloader createDownloader(Settings settings) throws InitComponentException {
    try {
         // 通过配置DownloaderModuleFactory自定义下载器
        DownloaderFactory factory = (DownloaderFactory) Class.forName(
                settings.getDownloaderModuleFactory()).newInstance();
        return factory.createDownloader(settings);
    } catch (Exception e) {
        throw new InitComponentException(e);
    }
}

interface DownloaderFactory {
     // 通过settings参数定义Downloader
    Downloader createDownloader(Settings settings);
}

public class CustomizeDownloader implements Downloader {}
```

自定义下载器需要实现DownloaderFactory和CustomizedDownloader。

注意一下，这里用到了**工厂方法模式**，Downloader的实现和Settings解耦，在DownloaderFactory中完成Downloader的具体实现代码。

#### 异步化下载&限速&终止条件

这三个问题都是Engine类需要解决的核心问题。
##### 代码
```Java
private void startSchedule() {
    LOGGER.info("{} begin to schedule !", spider.getSpiderName());

    while (true) {
        if (needBackout()) {
            waitOneSecond();
            continue;
        }
        Request request = scheduler.poll();
        if (request != null) {
            LOGGER.debug("{} request = {}", spider.getSpiderName(), request.getUrl());
            downloaderMiddlewareManager.processRequest(request);
        } else if (!isSpiderIdle()) {
            waitOneSecond();
        } else if (scheduler.isEmpty()) {
            LOGGER.info("{} stop", spider.getSpiderName());
            break;
        }
    }
}
```

##### 异步化下载
为了保证抓取效率，下载器必须并行下载多个页面。

和经典的Web服务器并发模型类似，主要有两种方式：1.单线程异步下载，回调处理页面数据的方法；2.多线程同步下载。

因为下载是个重IO操作，CPU操作很少，使用线程池的话，大多数时间线程都是在等待，采用多路IO复用这种方式明显是更为合适的。但是，因为下载操作需要downloader支持，所以具体实现还得看downloader是如何做的。比如我们需要使用公司提供的FetchServer，它只能利用线程池的方式来提供异步化，那么我们也只能用线程池这种方式了。

从上面的代码中，downloaderMiddlewareManager.processRequest(request);是个异步操作，不会等待方法返回才继续执行。
##### 限速
下载是异步的，但是不能不限速，倘若抓取速度太快是很容易被封禁的。在我们的框架中，限速有两种方式：1. scheduler限制url流出速度，2. downloader限制同时下载数。这两种方式从两个维度去限速，都可以通过settings来设置。

上面的代码中有两个方法，needTimeout和isSpiderIdle，我们先来看这两个方法的实现：

```Java
private boolean needTimeout() {
    return needTimeout();
}

// downloader.needTimeout
public boolean needTimeout() {
    return activeCount.intValue() > maxConcurrentCount;
}
```

因为我们会设置downloader的同时最大下载数，到达最大下载数时需要暂停，这里可以看到核心是：activeCount.intValue() > maxConcurrentCount;如果这句执行为true就需要暂停一下，等待activeCount值小于maxConcurrentCount后再继续执行。

另外，scheduler.poll();会根据setting中的设置定期吐出URL，而不是调用poll就会立即返回。

这两种限速方式不必同时发挥作用，只需要根据自己的需要按需定制。

##### 终止条件
终止条件是什么呢？当最后一个URL下载完成且没有产生下一个URL时，爬虫任务就可以结束了。

我们来看一下isSpideridle方法的实现：

```Java
private boolean isSpiderIdle() {
    return !downloader.isActive();
}

public boolean isActive() {
    return activeCount.intValue() > 0;
}
```
我们会在downloader中进行计数，只有activeCount也就是下载任务为0时，downloader才是idle的。这里需要提一下，因为我们的下载器是异步的，只有下载完成并且完成回调，activeCount才会减一。

在判断终止条件时，需要先判断downloader是否idle，然后判断scheduler是否empty。倘若反过来，则有可能提前终止爬虫任务。

#### Spider根据不同URL调用对应的解析方法
在上述多页面关联解析问题中，我们看到了Scrapy是如何解决多页面关联问题的。那么在Java中，我们应该怎么实现这种根据不同URL调用对应解析方法的呢？

先来看一下如何使用：

```Java
public ParseResults parse(Response response) {
    ParseResults results = new ParseResults();
    // parse code
    Request request = new Request(url, callback);
    return results;
}
```
Engine会解析parseResults，对于items会丢给pipeline去处理，对于request会丢给scheduler。注意到request的构造方法中有个callback，当Engine处理到这个request时，当页面下载完成后，Engine会调用这个request中的callback方法来解析页面内容。这样就实现了和Scrapy类似的功能。

我们借助Java8的Consumer类来实现，如果request没有定义callback，则调用默认解析方法parse来解析，具体实现：

```Java
ParseResults parseResponse(Response response) throws ParseException {
    ParseResults results = new ParseResults();
    // apply spider rules
    results.addRequests(applyRules(response));

    ParseResults spiderResults;
    Request request = response.getRequest();
    try {
        // 调用具体的解析方法
        spiderResults = invokeCallBack(request, response);
    } catch (Exception e) {
        throw new ParseException(e);
    }

    if (spiderResults != null) {
        results.addItems(spiderResults.getItems());
        results.addRequests(spiderResults.getRequests());
    }
    return results;
}

private ParseResults invokeCallBack(Request request, Response response) throws IllegalAccessException, NoSuchMethodException, InvocationTargetException {
    if (request.getCallBack() == null) {
        request.setCallBack(this :: parse);
    }
    return request.getCallBack().apply(response);
}
```

#### 选择器实现
选择器选用了JSoup和Xsoup，使用和Scrapy的类似，不再赘述。
#### 分布式爬虫
Scrapy是单机式爬虫框架，有较大的爬虫任务时会遇到性能瓶颈，分布式爬虫因运而生。分布式爬虫和单机式爬虫有什么区别呢？有以下几个：

1. 共享待抓取URL队列
2. 共享URL去重
3. 抓取结果合并
4. 支持新的机器加入，并且机器宕机不影响任务继续进行

根据上面的区别可以看出，核心点就是分布式的URL队列。基于Scrapy的分布式爬虫有Scrapy-redis，利用了redis来存储待抓取的URL队列，实现了分布式的爬虫。

在本框架中我们同样借助redis实现了RedisScheduler。如果爬虫任务想实现分布式抓取，可以在任务配置时把schduler配置为RedisScheduler即可。

### 取得的成效
目前利用本框架实现的爬虫任务已经有14个，相比较于之前没有爬虫框架的时候，开发新的爬虫任务能减少将近50%的代码量，大大提升了开发效率。