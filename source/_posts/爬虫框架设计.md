---
title: 爬虫框架设计
date: 2017-03-02 12:00:12
tags: [爬虫]
categories:
- 爬虫
---
### 起源
因为在公司做爬虫业务期间开发了一个新的基于Java的爬虫框架，故记录一下所思所想。

### 业务抽象
一个好的技术框架最重要的任务是设计出合理的抽象模型，对于爬虫框架也是一样。

如何设计爬虫系统的抽象模型呢？先看一个典型的爬虫案例。

[https://movie.douban.com/tag/喜剧](https://movie.douban.com/tag/%E5%96%9C%E5%89%A7) 是一个列表页，我们需要遍历这个列表页，进入每一个列表页中的电影详情页，并且抓取电影详情页中的电影数据。这样的一个任务，如果不用任何爬虫框架我们应该如何实现？其实，思路是很明确的，我们用伪代码来表示：

```Java
HTML html = getFirstTagHtml()
List<Movie> items = new ArrayList()
while (true) {
	List<String> movieDetailUrls = parseUrls(html)
	for (String url : movieDetailUrls) {
		HTML detail = getMovieDetailHtml()
		Movie movie = parseMovieDetaiPage(detail)
		items.add(movie)
	}
	if (!hasNextPage()) {
		break
	}
}

```
列表-详情页模式是一种非常常见的抓取模式，这个例子对设计爬虫系统的抽象模型有何启示呢？换言之，如果再去写另外一个列表-详情页模式的抓取，是否有一些可以复用的逻辑，不用二次开发了？

答案是肯定的，总结一下有以下几点：

1. 下载页面数据
	* 异步化
	* 失败自动重试
	* 代理、cookie、userAgent等设置
2. URL调度
	* url去重
	* 分布式
3. 解析
	* 自发现规则
	* 方法回调
4. 输出

我们对爬虫的流程进行分解之后，就会发现除了一些配置之外（代理，自发现规则等），只有解析是每个任务都避免不了的，而其余的都可以复用。这就是爬虫框架的意义所在。

### 架构图
框架设计自然避免不了架构图，先贴一张架构图：

![](https://doc.scrapy.org/en/latest/_images/scrapy_architecture_02.png)

上面的图片是[Scrapy](https://github.com/scrapy/scrapy)的架构图，作为爬虫界最知名的爬虫框架，Scrapy的架构在我看来无疑是最棒的。所以，没有必要在这块重复造轮子，照搬就是。

解释一下图中的几个重要模块：

* Downloader：下载器，负责下载页面内容，处理代理，cookie等设置；
* Spider：解析器，负责解析页面内容，输入是HTML的response，输出是结构化的Items；
* Scheduler：调度器，负责调度待抓取的URL以及URL去重；
* Item Pipelines：负责处理结构化的Items数据；
* Engine：引擎，负责将上面的这几个部分连接起来一起工作；

除此之外，还有几个重要类没有体现在架构图中：

* Settings：爬虫任务配置，任何配置都只会在Settings里配置；
* Crawler：每个爬虫任务都对应一个Crawler

### 为什么要开发新的爬虫框架
按理说Scrapy已经是很不错的爬虫框架了，那么为什么又要重新开发一个呢？原因有以下几个：

1. Scrapy不支持分布式爬虫；
2. 公司的后端环境是Java环境，用Python会带来很多的兼容问题；
3. Java届最有名的爬虫框架WebMagic提供的爬虫模型不够好；
4. 爬虫框架这个轮子并不难造，并且如果是自己造的，二次扩展显然更加简便；

#### WebMagic的问题(不了解WebMagic的同学可以忽略)
先上一张架构图

![](http://code4craft.github.io/images/posts/webmagic.png)
##### PageProcessor定义混乱
PageProcessor，顾名思义，是处理页面解析内容的，对应到Scrapy中的Spider。但是，它还有两个其它的任务：1. 配置site，例如编码、抓取间隔、重试次数等；2. 启动整个爬虫任务。举个例子：

```Java
public class DoubanMovieDetailPageProcessor implements PageProcessor {

	// 配置site信息
	private Site site = Site.me().setRetryTimes(3).setSleepTime(1000);
	
	@Override
	public void process(Page page) {
		抽取页面逻辑
	}
	
	// getSite方法很多余
	@Override
    public Site getSite() {
        return site;
    }
	
	public static void main(String[] args) {
		通过Spider类启动爬虫任务
	}
}
```

很明显，无论是配置Site信息，还是启动爬虫任务都不是PageProcessor的职责。在Scrapy中，配置Site信息有Settings设置，启动爬虫任务有Crawler类，而Scrapy中的PageProcessor，也就是Spider类，只处理页面抽取，这样做逻辑要通顺很多。
##### Page类中不应该有ResultItems
ResultItems是抽取页面数据后产生的结构化数据，而Page对象是下载后的页面对象，Page类中有个ResultItems没有道理。应当和Scrapy一样，ResultItems是process方法的返回值，然后交给Pipeline处理。
##### 多页面关联解析问题
类似列表页-详情页这种模式的抓取任务，我们需要解析两个页面，列表页和详情页，用WebMagic怎么实现呢？

```Java
@override
public void process(Page page) {
	if (page.getUrl().regex(URL_LIST).match()) {
    	page.addTargetRequests(page.getHtml().xpath("//div[@class=\"articleList\"]").links().regex(URL_POST).all());
    	page.addTargetRequests(page.getHtml().links().regex(URL_LIST).all());
    	//文章页
	} else {
	    page.putField("title", page.getHtml().xpath("//div[@class='articalTitle']/h2"));
	    page.putField("content", page.getHtml().xpath("//div[@id='articlebody']//div[@class='articalContent']"));
	    page.putField("date",
	            page.getHtml().xpath("//div[@id='articlebody']//span[@class='time SG_txtc']").regex("\\((.*)\\)"));
	}
}
```
在Scrapy中是如何实现的？

```Python
def parse(self, response):
    urls = response.xpath('//div[contains(@class, "open-pt")]//a/@href').extract()
    for url in urls:
        yield scrapy.Request(url, dont_filter = True, callback = self.parse_star)

def parse_star(self, response):
    schedule_url = response.xpath(u'//a[contains(@title, "全部行程")]/@href').extract_first()
    for month in xrange(1, 2):
        yield scrapy.Request('%s2016/%d/#blanking' %(schedule_url, month), callback = self.parse_star_schedule)

def parse_star_schedule(self, response):
	解析行程页内容
	
```
Scrapy每一个页面都对应了一个解析方法，而WebMagic则是一个Else if的判断，Scrapy明显更优。

##### 配置自发现的规则不够直接 
在PageProcessor中配置自发现的规则是这么写的：

```Java
page.addTargetRequests(page.getHtml().links().regex("(https://github\\.com/\\w+/\\w+)").all());
```
和Scrapy相比，高下立判:

```Java
rules = (
	Rule(LinkExtractor(allow=('category\.php', ), deny=('subsection\.php', ))),
	Rule(LinkExtractor(allow=('item\.php', )), callback='parse_item'),
)
```
配置这种规则不是PageProcessor的职责，而是Site的职责，放在PageProcessor中明显不合理。

##### 并发下载问题
这是一个经典问题，在此不多说了，下载是一个IO时间远大于CPU时间的场景，用IO多路复用更合适，而不是多线程模型。

#### WebMagic的优点
说了这么多缺点，WebMagic还是有优点的。

1. 上手很快，文档比较全
2. Downloader，Scheduler，Pipeline很容易扩展
3. 代码结构比较清晰

### 开发爬虫框架时的几个难题
前面也提到了，Scrapy作为业界最佳爬虫框架，大的方面比如架构照搬就好了。但是毕竟语言有差异，很多实现细节是不能用相似方案实现的，下面主要讲几个开发爬虫框架时碰到的问题。

#### 动态初始化组件
Downloader、Scheduler和Pipeline的实现类可以由使用方替换，所以需要在爬虫任务启动时动态生成。我们希望使用方通过配置类名的方式告知实现类，而不是new出来对象再传递给Engine。这样Engine也更加容易管理各个组件，避免对象逸出。

```Java
Engine(Settings settings) throws InitComponentException {
    this.downloader = DownloaderCreator.createDownloader(settings);
    this.scheduler = SchedulerCreator.createScheduler(settings);
    this.pipeline = PipelineCreator.createPipeline(settings);
}
```
以DownloaderCreateor为例来说明初始化过程。

```Java
public static Downloader createDownloader(Settings settings) throws InitComponentException {
    try {
    	 // 通过配置DownloaderModuleFactory自定义下载器
        DownloaderFactory factory = (DownloaderFactory) Class.forName(
                settings.getDownloaderModuleFactory()).newInstance();
        return factory.createDownloader(settings);
    } catch (Exception e) {
        throw new InitComponentException(e);
    }
}

interface DownloaderFactory {
	 // 通过settings参数定义Downloader
    Downloader createDownloader(Settings settings);
}

public class CustomizeDownloader implements Downloader {}
```

自定义下载器需要实现DownloaderFactory和CustomizedDownloader。

注意一下，这里用到了**工厂方法模式**，Downloader的实现和Settings解耦，在DownloaderFactory中完成Downloader的具体实现代码。

#### 异步化下载&限速&终止条件
##### 异步化下载
为了保证抓取效率，下载器必须并行下载多个页面。

并行下载有两种方式：1.单线程异步下载，回调处理页面数据的方法；2.多线程同步下载。

